# Análisis de Funciones Necesarias para el Lexer

Para las tareas del lexer que describes, necesitarás principalmente funciones de **gestión de memoria y manipulación de strings**. Aquí están las funciones permitidas que usarás:

## Funciones Permitidas que Necesitarás:

1. **`malloc`** - Para asignar memoria dinámica para tokens y estructuras
2. **`free`** - Para liberar la memoria asignada
3. **`write`** - Solo si necesitas debug (opcional)
4. **`readline`** - Para obtener la entrada del usuario (ya viene antes del lexer)

**Nota importante:** El lexer NO necesita funciones de sistema como `fork`, `pipe`, `execve`, etc. Esas las usarás en el parser y el executor. El lexer es puramente **análisis y estructuración de texto**.

---

# Plan Detallado para Implementar el Lexer

## **PASO 1: Definir las Estructuras de Datos**

Antes de tokenizar, necesitas decidir **cómo almacenar** los tokens.

### Sub-paso 1.1: Crear la estructura de un token
Cada token debe contener:
- **El contenido** (la palabra o símbolo en sí)
- **El tipo** (¿es un comando? ¿un operador? ¿un argumento?)
- **Un puntero al siguiente token** (si usas lista enlazada)

**Explicación:** Un token es la unidad mínima de información. Por ejemplo, en `echo hello | cat`, tendrías los tokens: `"echo"`, `"hello"`, `"|"`, `"cat"`.

### Sub-paso 1.2: Definir los tipos de tokens
Necesitas categorizar qué tipo de elemento es cada token:
- **WORD** (palabra normal: comandos, argumentos, rutas)
- **PIPE** (el carácter `|`)
- **REDIRECT_IN** (el carácter `<`)
- **REDIRECT_OUT** (el carácter `>`)
- **REDIRECT_APPEND** (los caracteres `>>`)
- **REDIRECT_HEREDOC** (los caracteres `<<`)
- **ENV_VAR** (variables de entorno como `$USER`)
- **EXIT_STATUS** (el símbolo especial `$?`)

**Explicación:** Clasificar los tokens facilita las etapas posteriores. El parser sabrá inmediatamente qué hacer con cada token según su tipo.

---

## **PASO 2: Leer y Preparar la Entrada**

### Sub-paso 2.1: Obtener la línea de entrada
La función `readline()` te da un string con la entrada del usuario. Este string es tu "materia prima".

**Explicación:** `readline("minishell> ")` devuelve algo como `"echo hello | cat"`. Ahora necesitas descomponerlo.

### Sub-paso 2.2: Validar que la entrada no esté vacía
Antes de tokenizar, verifica que el usuario haya escrito algo. Si es NULL o solo espacios, no hagas nada.

**Explicación:** Evitas procesar entradas inútiles y posibles errores.

---

## **PASO 3: Implementar la Lógica de Tokenización**

Este es el núcleo del lexer. Aquí recorres el string carácter por carácter y construyes tokens.

### Sub-paso 3.1: Crear un índice para recorrer el string
Necesitas una variable que vaya avanzando por la entrada, carácter a carácter.

**Explicación:** Piensa en el string como un array. Necesitas saber "en qué posición estoy ahora mismo".

### Sub-paso 3.2: Saltar espacios en blanco
Antes de crear un token, ignora todos los espacios, tabs o saltos de línea.

**Explicación:** Los espacios separan tokens, pero no son tokens en sí mismos. `"echo   hello"` debe generar dos tokens: `"echo"` y `"hello"`, no tres.

### Sub-paso 3.3: Identificar el tipo de token según el primer carácter
Aquí empieza la clasificación:

#### **Si el carácter es `|`:**
- Es un token de tipo **PIPE**
- Avanzas una posición en el string
- Creas el token con contenido `"|"`

#### **Si el carácter es `<`:**
- Miras el siguiente carácter:
  - Si es otro `<`, es **REDIRECT_HEREDOC** (`<<`)
  - Si no, es **REDIRECT_IN** (`<`)
- Avanzas 1 o 2 posiciones según corresponda

#### **Si el carácter es `>`:**
- Miras el siguiente carácter:
  - Si es otro `>`, es **REDIRECT_APPEND** (`>>`)
  - Si no, es **REDIRECT_OUT** (`>`)
- Avanzas 1 o 2 posiciones según corresponda

#### **Si el carácter es `'` (comilla simple):**
- Todo lo que esté entre comillas simples es **un solo token**
- Las comillas protegen todo literalmente (no se expanden variables)
- Ejemplo: `'$USER'` → el token contiene exactamente `$USER`

**Explicación detallada de las comillas simples:**
1. Detectas la comilla de apertura `'`
2. Avanzas hasta encontrar la comilla de cierre `'`
3. Todo el contenido intermedio es **un único token de tipo WORD**
4. Las comillas NO forman parte del contenido del token (las eliminas)

#### **Si el carácter es `"` (comilla doble):**
- Todo entre comillas dobles es **un solo token**
- Pero **SÍ se deben expandir** las variables `$` dentro
- Ejemplo: `"$USER"` → el token debe contener el valor de USER

**Explicación detallada de las comillas dobles:**
1. Detectas la comilla de apertura `"`
2. Avanzas leyendo caracteres:
   - Si encuentras `$`, marcas que hay una variable a expandir
   - Sigues hasta la comilla de cierre `"`
3. Creas un token de tipo WORD
4. Más adelante (en expansión) sustituirás `$USER` por su valor

#### **Si el carácter es `$`:**
- Puede ser:
  - **`$?`** → Token de tipo EXIT_STATUS
  - **`$VARIABLE`** → Token de tipo ENV_VAR
- Lees hasta encontrar un carácter que no sea alfanumérico o `_`

**Explicación:** Las variables de entorno son secuencias que empiezan con `$` y continúan con letras, números o guiones bajos. `$HOME` es válido, `$123` también, pero `$-` no.

#### **Si el carácter es cualquier otra cosa:**
- Es el inicio de una **WORD** (palabra normal)
- Lees caracteres hasta encontrar:
  - Un espacio
  - Un operador (`|`, `<`, `>`)
  - Una comilla
  - Fin del string

**Explicación:** Una palabra es cualquier secuencia de caracteres que no son metacaracteres especiales. Ejemplos: `echo`, `hello`, `/usr/bin/ls`, `archivo.txt`.

---

## **PASO 4: Crear y Almacenar los Tokens**

### Sub-paso 4.1: Asignar memoria para cada token
Cuando identificas un token (su tipo y su contenido), usas `malloc` para crear la estructura.

**Explicación:** Cada token necesita espacio en memoria para:
- Un puntero al contenido (string)
- Un enum o int para el tipo
- Un puntero al siguiente token (si usas lista enlazada)

### Sub-paso 4.2: Copiar el contenido al token
El contenido del token debe ser una copia independiente del string original.

**Explicación:** No puedes simplemente guardar un puntero al string de `readline`, porque ese string podría ser liberado o modificado. Necesitas tu propia copia.

### Sub-paso 4.3: Enlazar el token a la lista
Si usas lista enlazada:
- El primer token es la cabeza
- Cada nuevo token se añade al final
- El último token apunta a NULL

**Explicación:** La lista enlazada te permite recorrer todos los tokens en orden secuencial más adelante, cuando el parser los procese.

---

## **PASO 5: Gestionar Casos Especiales**

### Sub-paso 5.1: Detectar comillas sin cerrar
Si encuentras una comilla de apertura pero llegas al final del string sin encontrar la de cierre, debes:
- Mostrar un error al usuario
- No crear tokens
- Retornar NULL o un indicador de error

**Explicación:** El subject dice que NO debes interpretar comillas sin cerrar. Bash muestra `>` esperando más input, pero tú simplemente rechazas la entrada.

### Sub-paso 5.2: Ignorar caracteres no especificados
Si encuentras `\` o `;`, el subject dice que **no los interpretes**. Trátalos como caracteres normales dentro de palabras.

**Explicación:** No implementas escapes como en bash completo. Si alguien escribe `echo\ hello`, los tokenizas como `"echo\"` y `"hello"` (aunque podrías tratarlo como error también).

---

## **PASO 6: Retornar la Lista de Tokens**

### Sub-paso 6.1: Verificar que la tokenización fue exitosa
Si todo salió bien, tienes una lista de tokens lista para el parser.

### Sub-paso 6.2: En caso de error, liberar memoria
Si algo falló (comillas sin cerrar, malloc falló, etc.):
- Libera todos los tokens ya creados
- Libera todas las memorias asignadas
- Retorna NULL

**Explicación:** No puedes dejar memoria colgando. Si falla algo a mitad de proceso, debes limpiar todo lo hecho hasta ese momento.

---

## **PASO 7: Testing y Validación**

### Sub-paso 7.1: Probar con entradas simples
Empieza con casos básicos:
- `echo hello` → 2 tokens: WORD("echo"), WORD("hello")
- `ls -la` → 2 tokens: WORD("ls"), WORD("-la")

### Sub-paso 7.2: Probar con operadores
- `echo hello | cat` → 3 tokens: WORD("echo"), WORD("hello"), PIPE, WORD("cat")
- `cat < file` → 3 tokens: WORD("cat"), REDIRECT_IN, WORD("file")

### Sub-paso 7.3: Probar con comillas
- `echo 'hello world'` → 2 tokens: WORD("echo"), WORD("hello world")
- `echo "$USER"` → 2 tokens: WORD("echo"), WORD("$USER") con marca de expansión

### Sub-paso 7.4: Probar casos de error
- `echo "hello` → Error: comillas sin cerrar
- ` ` (solo espacios) → No crear tokens

---

# Resumen de la Lógica General

1. **Recibir** la línea de entrada
2. **Recorrer** carácter por carácter
3. **Identificar** qué tipo de elemento es cada secuencia
4. **Crear** estructuras de datos (tokens) con esa información
5. **Almacenar** los tokens en una lista ordenada
6. **Retornar** la lista para que el parser la procese

El lexer **NO ejecuta nada**, **NO expande variables todavía** (solo las identifica), **NO valida sintaxis compleja**. Solo descompone texto en piezas clasificadas.